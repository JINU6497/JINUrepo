{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "데이터 전처리 및 Pre-Trained Embedding Vector를 이용한 Vocabulary 생성.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPY2Tm5R1x7cOItbPF9bDzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JINU6497/JINUrepo/blob/master/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC_%EB%B0%8F_Pre_Trained_Embedding_Vector%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_Vocabulary_%EC%83%9D%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOF53aE0MM56",
        "outputId": "e4db448a-d4bc-41c0-c684-2c03ef4be30e"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "TEXT=data.Field(batch_first=True,\n",
        "                fix_length=500,\n",
        "                tokenize=str.split,\n",
        "                pad_first=True,\n",
        "                pad_token='[PAD]',\n",
        "                unk_token='[UNK]'\n",
        "                )\n",
        "# fix_label: Sentence의 길이를 미리 제한하는 옵션\n",
        "# tokenize: Tokenize를 설정하는 옵션. 기본값은 띄어쓰기 기반의 파이썬의 string.split 함수\n",
        "# pad_first: fix_length 대비 짧은 문장의 경우에는 padding 해줘야 하는데, padding을 앞에서 해 줄 것인지에 대한 옵션\n",
        "# pad_token: 설정한 padding에 대한 특수 Token 설정\n",
        "# unk_token: Token dictionary에 없는 token이 나왔을 경우 해당 token을 표현하는 특수 token\n",
        "\n",
        "LABEL=data.LabelField(dtype=torch.float)\n",
        "# dtype: 가져올 데이터에 대한 Type 설정 옵션\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(text_field=TEXT, label_field=LABEL)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:00<00:00, 89.0MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xdC-lfOQfLs",
        "outputId": "6e94178d-fcb4-410e-8f1e-4ba3513f235a"
      },
      "source": [
        "# Data length\n",
        "print(f'Train Data Length: {len(train_data.examples)}')\n",
        "print(f'Test Data Length: {len(test_data.examples)}')\n",
        "\n",
        "# Data field\n",
        "print(train_data.fields)\n",
        "\n",
        "# Data Sample\n",
        "print('---- Data Sample ----')\n",
        "print('Input: ')\n",
        "print(''.join(vars(train_data.examples[1])['text']),'\\\\n')\n",
        "print('Label: ')\n",
        "print(vars(train_data.examples[1])['label'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data Length: 25000\n",
            "Test Data Length: 25000\n",
            "{'text': <torchtext.legacy.data.field.Field object at 0x7fa09c728050>, 'label': <torchtext.legacy.data.field.LabelField object at 0x7fa0964297d0>}\n",
            "---- Data Sample ----\n",
            "Input: \n",
            "AgreatBritishIndymovie!Fantasticchemistrybetweenthe3maincharactersmakeforsomehilariousdrug-fuelledsetpiecesthatCheechandChongwouldbeproudof.GreattoseePhilDanielsbackonthebigscreen(evenifhehasswappedsidessinceQuadrophenia!)andGaryStretchissurprisinglygoodandatreatfortheladies!Lovedthefinalfightscenewithit'snodtoZuluandnowIknowwhathappenedtoArthurBrownafterhesethimselfonfireonTopofthePops!...he'snotacting....hereallyisabona-fideBritishhippie!!!Youdon'thavetobeabikertoenjoythisandit'sstraightintomyFridaynightpost-pubrepeatviewingcollection.<br/><br/>Givethisfilmagoandyouwon'tbedisappointed. \\n\n",
            "Label: \n",
            "pos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZf0fQH_OtOD"
      },
      "source": [
        "data.examples를 이용하여 데이터의 개수를 확인할 수 있고, vars()함수를 이용하여 데이터의 값을 직접 확인. 이때 Text data는 Tokenize를 하기 전에 Data를 살펴보고 cleansing작업을 해야 한다. 이는 다음과 같이 진행한다. (이는 Field의 preprocessing옵션을 이용하여도 미리 작업 가능)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQS6Ezoq4Gmu"
      },
      "source": [
        "import re\n",
        "\n",
        "def PreProcessingText(input_sentence):\n",
        "  input_sentence = input_sentence.lower()  # 소문자화\n",
        "  input_sentence = re.sub('<{^>]*>', repl='', string=input_sentence)  # \"<br />\" 처리\n",
        "  input_sentence = re.sub('[!\"#$%&\\\\()*+,-./:;<=>?@[\\\\\\\\]^_{|}~]',repl='',string=input_sentence)  # 특수 문자 처리(\"'\" 제외)\n",
        "  input_sentence = re.sub('\\\\s+',repl='',string=input_sentence)  # 연속된 띄어쓰기 처리\n",
        "\n",
        "  if input_sentence:\n",
        "    return input_sentence\n",
        "\n",
        "for example in train_data.examples:\n",
        "  vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split( )\n",
        "\n",
        "for example in test_data.examples:\n",
        "  vars(example)['text'] = PreProcessingText(' '.join(vars(example)['text'])).split( )\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHPL1Kp2Egrj",
        "outputId": "78b0846d-f4a1-4c89-b50f-156e11fbe806"
      },
      "source": [
        "# 주어진 Data를 이용하여 Token Vocabulary를 만드는 과정\n",
        "\n",
        "# pre-trained\n",
        "TEXT.build_vocab(train_data, min_freq =2,  # min_freq: Vocab에 해당하는 Token에 최소한으로 등장하는 횟수에 제한을 두는 것\n",
        "                 max_size = None,  # max_size: Token의 최소 등장 횟수로 Vocab의 Size조절을 하는 것 외에 전체 Vocab Size 자체에도 제한을 두는 것\n",
        "                 vectors = \"glove.6B.300d\"  # vectors: pre-trained vector를 가져와 Vocab에 세팅. 원하는 Embedding을 정해 string 형태로 설정\n",
        "                 )\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 399999/400000 [00:47<00:00, 8400.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PXnHiNrG37T",
        "outputId": "652c196b-5595-4ef1-8113-158ff6e4f1dd"
      },
      "source": [
        "# Vocab에 대한 정보 확인하기\n",
        "\n",
        "print(f'Vocab Size:{len(TEXT.vocab)}')\n",
        "\n",
        "print('Vocab examples: ')\n",
        "\n",
        "for idx,(k,v) in enumerate(TEXT.vocab.stoi.items()):\n",
        "  if idx >=10:\n",
        "    break\n",
        "  print('\\\\t',k,v)\n",
        "\n",
        "print('--------------------------------------------')\n",
        "\n",
        "# Label에 대한 정보 확인\n",
        "\n",
        "print(f'Label Size:{len(LABEL.vocab)}')\n",
        "\n",
        "print('Label Examples: ')\n",
        "for idx,(k,v) in enumerate(LABEL.vocab.stoi.items()):\n",
        "  print('\\\\t',k,v)\n",
        "\n",
        "# Check embedding vector\n",
        "\n",
        "print(TEXT.vocab.vectors.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size:96\n",
            "Vocab examples: \n",
            "\\t [UNK] 0\n",
            "\\t [PAD] 1\n",
            "\\t howhasthispieceofcrapstayedontvthislong?it'sterrible.itmakesmewanttoshootsomeone.it'ssofakethatitisactuallyworsethana1940ssci-fimovie.i'dratherhaveastrokethanwatchthisnonsense.irememberwatchingitwhenitfirstcameout.ithought,heythiscouldbeinteresting,thenifoundouthowabsolutely,insanely,ridiculouslystupiditreallywas.itwassobadthatiactuallytookoutmypocketknifeandstuckmyhandtothetable.<br/><br/>pleasepeople,stopwatchingthisandallotherrealityshows,they'rethetrashthatisjammingthenetworksandcancelingqualityprogrammingthatrequiressomethoughttocreate. 2\n",
            "\\t thisshowcomesupwithinterestinglocationsasfastasthetravelchannel.itisbilledasrealitybutinactualityitispureprimetimesoapopera.it'striestouseexoticlocalesasafacadetobringpeopleintoaphonycontest&thenproceedstohookviewersonthecontestantssoapoperastyle.<br/><br/>italsoborrowsfromanearlycbsgameshowpioneer-beattheclock-byinventingsituationsforitscontestantstotry&overcome.thenitrewardsthewinnermoney.iftheycanspiceitupwithalittleinteractionbetweenthecharacters,evenbetter.whilethegameformatisinslowmotionversusbeattheclock-therealaccomplishmentofthisseriesistoescapereality.<br/><br/>thisshowhaselementsofseveraltypesofsuccessfulpastprograms.realitytelevision,hardly,butifyourhookedonthecontestants,localeorcontest,thisisyourcupoftea.ifyournot,thisentireseriesisasisay,driveldrippingwithgravy.itisanothershowhidingbehindtherealitylabelwhichisthetrenditstartedin2000.<br/><br/>itisslick&wellproduced,soitmightlastawhileyet.afterall,sodore-runsofgilligan'sisland,greenacres,thebeverlyhillbillies&thebradybunch.thisjustdoesn'temployprofessionalactors.theintelligencelevelisaboutthesame. 3\n",
            "\\t whenigotthismoviefreefrommyjob,alongwiththreeothersimilarmovies..iwatchedthenwithverylowexpectations.nowthismovieisn'tbadperse.yougetwhatyoupayfor.itisataleoflove,betrayal,lies,sex,scandal,everythingyouwantinamovie.definitelynotahollywoodblockbuster,butforcheapthrillsitisnotthatbad.iwouldprobablyneverwatchthismovieagain.inanutshellthisisthekindofmoviethatyouwouldseeeitherverylateatnightonalocaltelevisionstationthatisjustwantingtotakeupsometime,oryouwouldseeitonasundayafternoononalocaltelevisionstationthatistryingtotakeupsometime.despitethebadacting,clichélines,andsubparcamerawork.ididn'thavethedesiretoturnoffthemovieandpretendlikeitneverpoppedintomydvdplayer.thestoryhasbeendonemanytimesinmanymovies.thisoneisnodifferent,nobetter,noworse.<br/><br/>justyouraveragemovie. 4\n",
            "\\t youdorealizethatyou'vebeenwatchingtheexactsameshowforeightyears,right?icouldunderstandtheinitialcuriosityofseeingstrangersco-existonanisland,butyou'dthinkthatafterwatchingunkempt,stink-ladenedheroesrunroughshodthroughthebushwithaneggonaspoonforhalfadecadewouldbeenoughtogetyoutocommittosomethingalittlemoreoriginal(andinteresting).<br/><br/>andi'mnotevenspeakingoftheshowsvaliditywhichfortherecordifindquestionable.it'sjusthardtosuspenddisbelieffor\"bushybill\"eatingaratwhentheentirecrewofproducersandcamerapeoplearehousedinanairconditionedmake-shiftbio-domesippingfrostymochcinno'swithmoxy.<br/><br/>what'stheappealhere?idon'tcareaboutthesepeopleortheirmeanderinglives.ijustdon'tgetit.butifyoudofindyourselfbeingcaptivatedbyhairy,unwashedpeople,isuggestyouturnoffyourtvandjusttakeatriptoyourlocalbusstationwhereyoucanseepeoplelikethisintheirtruehabitat.theycallthemhomelesspeople,andfreeofcharge,youcansitbackandmarvelintheiruncannyabilitytoretrievevariouscigarettedebrisfromaplethoraofgarbagecanisters,eventuallystriking\"pay-dirt\"andfashioningahomemadedr.frankenstein-styledcancer-stick,allthewhilebeggingpeopleforchangeforfoodwhenthestinkof\"aquavelva\"ontheirbreathisenoughtosuggestotherwise.andthebestpart?muchlikesurvivor,everyweekonememberofthetribe\"leaves\"the\"island\"whentheyareunceremoniouslysentpackingtothelocalinstitutionwhenthefrighteningunmedicatedstateoffull-blownschizophreniakicksintogear!nowthat'sentertainment! 5\n",
            "\\t 'deadletteroffice'isalow-budgetfilmaboutacoupleofemployeesoftheaustralianpostalservice,strugglingtorebuildtheirdamagedlives.unfortunately,theactingispoorandthelinksbetweenthecharacters'pastmisfortunesandpresentmindsetsareclumsilyandover-schematicallyrepresented.what'smostdisappointingofall,however,istheportrayalislifeintheofficeofthefilm'stitle:there'snomechanisationwhatsoever,andit'squiteimpossibletoascertainwhatanyofthestaffreallydoforaliving.granted,partoftheplotisthattheofficeisthreatenedwithclosure,butthissortofofficesurelyclosedinthe1930s,ifitevertrulyexisted.it'sashame,asthefilm'soveralltoneispoignantandwry,andthere'ssomepromiseinthescenario:butfewofthedetailsconvince.overall,itfeelstheworkofsomeonewhohasn'tactuallyexperiencedmuchofreallife;astudentfilm,withaconceptandanoutline,butsadlylittleelse. 6\n",
            "\\t .......playingkaddiddlehopper,colsanfernando,etc.themanwasprettywiderangingandascream.ilovewatchinghiminteractw/amandablake,ordonknottsorwhomever--heclearlywashavingaballandithinkhemadeiteasieronhisguestsaswell--solongastheyknewaheadoftimeitwasn'tadisciplined,19takekindofproduction.relaxandbeloosewasclearlythenameofthegamethere.<br/><br/>heremindsmeofguyslikemiltonberle,bennyhill,maybejerrylewissometoo.greattiming,ancientgagsthatkeptaudiencesinstitchesfordecades,sheerenjoymentaboutwhathewasdoing.hissadlittleclownheplayedwasgoodtoo--butinatouchingmanner.<br/><br/>personallyithinkhe'sgreat,havingjustboughtatwodvdsetofhisshowsfrom'61orso,itbringshisstuffbackinafondwayforme.icanrememberseeinghimontvattheendofhisrunwhenhewaswindinguptheseriesin1971orso.<br/><br/>checkthisoutifyouareafanorcurious.hewasariot. 7\n",
            "\\t <br/><br/>backinhisyouth,theoldmanhadwantedtomarryhisfirstcousin,buthisfamilyforbidit.manydecadeslater,theoldmanhasraisedthreechildren(twoboysandonegirl),andallowshissonanddaughtertomarryandhavechildren.soon,thesisterisboredwithbrother#1,andjumpsinthebedofbrother#2.<br/><br/>onemightthinkthatthethreesiblingsarestucksomewhereonaremoteisland.butno--theyareupperclasseuropeansgoingtocollegeandbusyinthesocialworld.<br/><br/>neverdoweseeaflirtatiousmomentbetweenanynon-relatedfemaleandthetwobrothers.neverdoweseeanyflirtatiousmomentbetweenanynon-relatedmaleandtheonesister.allflirtatiousmomentsaresharedbetweenonlybetweenthebrothersandsister.<br/><br/>theweakestpartofgladiatorwastheincestthing.theyoungemperorcommoduswouldhavehundredsofslavegirlsandacityfullofmarriage-mindedgirlsalloverhim,butno--heonlywantedhissister?ifmovieincestisyourcupoftea,thensunshinewill(slowly)thrillyoutonoend. 8\n",
            "\\t afterlosingtheemmyforherperformanceasmamaroseinthetelevisionversionofgypsy,bettewonanemmythefollowingyearforbettemidler:divalasvegas,aliveconcertspecialfilmedforhbofromlasvegas.midler,whohasbeenperformingliveonstagesincethe1970's,provesthatsheisstilloneofthemostelectrifyingliveperformersinthebusiness.fromheropeningnumber,herclassic\"friends\",whereshedescendsfromthewingsatopabeautifulpropcloud,bettecommandsthestagewithstyleandcharismafromarap-stylednumbercalled\"ilookgood\"shethenprovesthatshehasawaywithajokelikefewotherperformersinthisbusinessassheseguesherwaythroughavarietyofmusicalselections.thesectionoftheshowwhereshesalutesburlesquegoesonalittletoolongbutshedoesmanagetoincorporateheroldsophietuckerjokesheretogoodadvantage(eventhoughsheactuallyforgetsonejokeinthemiddleoftellingit,butherad-libbinguntilsheremembersitishysterical).bettealsotreatsusto\"rose'sturn\"fromgypsyandthetitletunefromhersmashfilmtheroseaswellasashamelessplugforherhitmoviethefirstwivesclub.shebringsthehousedownneartheendwith\"staywithme,baby\"fromtheroseandheronly#1hitrecord,\"windbeneathmywings\"frombeaches.it'sadazzlingeveningofmusicalcomedyentertainmentandformidlerfans,it'samust. 9\n",
            "--------------------------------------------\n",
            "Label Size:2\n",
            "Label Examples: \n",
            "\\t neg 0\n",
            "\\t pos 1\n",
            "torch.Size([96, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54x29HIXVNcq"
      },
      "source": [
        "# Validation Set 구분과 Iterator를 이용하여 Batch Data를 만들면 모델 학습을 위한 데이터 설정은 끝\n",
        "\n",
        "import random\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(0),split_ratio=0.8) # random.seed(): 무작위가 포함된 공정의 완전한 재현을 가능하게끔 만들어주는 것\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(datasets=(train_data, valid_data, test_data),\n",
        "                                                                           batch_size=30, device=device)"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}